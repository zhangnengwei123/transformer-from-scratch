{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e746b28-01fc-42c0-92a2-4a1f4d9d3118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 准备数据\n",
    "# 先用这个数据作为训练数据，模型训练完成之后，\n",
    "# 向模型输入这个source_sentence,模型能输出target_sentence 那就很满足。\n",
    "from transformer.MyTransformer import MyTransformer\n",
    "\n",
    "source_sentences = [\"你真是个傻逼\", \"你咋这么能呢\", \"我将带头冲锋\", \"在东南亚打自由搏击\", \"铃儿响叮当\"]\n",
    "target_sentences = [\"你真是个小可爱\", \"你就是个菜鸡\", \"我先撤你们上\", \"这货真牛逼\", \"今天真开心\"]\n",
    "\n",
    "sequence_empty_pad = \"[PAD]\"\n",
    "sequence_start_pad = \"[CLS]\"\n",
    "sequence_end_pad = \"[SEP]\"\n",
    "\n",
    "sequence_max_length = 12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b5c7595-97a6-47e7-97d0-926b044c6e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "字符字典:\n",
      "{'[CLS]': 0, '[PAD]': 1, '[SEP]': 2, '上': 3, '东': 4, '个': 5, '么': 6, '亚': 7, '今': 8, '们': 9, '你': 10, '傻': 11, '儿': 12, '先': 13, '冲': 14, '击': 15, '南': 16, '叮': 17, '可': 18, '呢': 19, '咋': 20, '响': 21, '在': 22, '天': 23, '头': 24, '将': 25, '小': 26, '就': 27, '带': 28, '开': 29, '当': 30, '心': 31, '我': 32, '打': 33, '搏': 34, '撤': 35, '是': 36, '爱': 37, '牛': 38, '由': 39, '真': 40, '能': 41, '自': 42, '菜': 43, '货': 44, '这': 45, '逼': 46, '铃': 47, '锋': 48, '鸡': 49}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 建立词典\n",
    "# 提取所有字符\n",
    "all_characters = set()\n",
    "all_characters.add(sequence_empty_pad)\n",
    "all_characters.add(sequence_start_pad)\n",
    "all_characters.add(sequence_end_pad)\n",
    "for sentence in source_sentences + target_sentences:\n",
    "    for char in sentence:\n",
    "        all_characters.add(char)\n",
    "\n",
    "# 去重并排序\n",
    "sorted_characters = sorted(all_characters)\n",
    "\n",
    "vocabulary_size = len(sorted_characters)  # 词汇表大小\n",
    "\n",
    "# 构建字典\n",
    "char_to_index = {char: idx for idx, char in enumerate(sorted_characters)}\n",
    "index_to_char = {idx: char for idx, char in enumerate(sorted_characters)}\n",
    "\n",
    "pad_index = char_to_index[sequence_empty_pad]\n",
    "\n",
    "\n",
    "def tokenize(sequence):\n",
    "    return [token for token in sequence]\n",
    "\n",
    "\n",
    "def convert_tokens_to_ids(tokens):\n",
    "    return [char_to_index[token_item] for token_item in tokens]\n",
    "\n",
    "\n",
    "def convert_ids_to_tokens(ids):\n",
    "    return [index_to_char[id_item] for id_item in ids]\n",
    "\n",
    "\n",
    "# 打印结果\n",
    "print(\"字符字典:\")\n",
    "print(char_to_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76372f5e-231d-4f53-ae88-39e055e5f544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "012e7149-2fba-4151-9e20-d83d53a1f190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = MyTransformer(i_vocabulary_size=vocabulary_size,\n",
    "                      t_vocabulary_size=vocabulary_size,\n",
    "                      src_pad_idx=pad_index,\n",
    "                      tgt_pad_idx=pad_index,\n",
    "                      d_model=16,\n",
    "                      num_heads=8,\n",
    "                      num_encoder_layers=6,\n",
    "                      num_decoder_layers=6,\n",
    "                      dim_feedforward=64,\n",
    "                      dropout=0.1\n",
    "                      )\n",
    "\n",
    "batch_size = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a653e87d-3a21-4470-adb7-5e48e9a5dc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 0, 4])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_tokens_batch():\n",
    "    ixs = torch.randint(0, len(source_sentences), (batch_size,))\n",
    "    print(ixs)\n",
    "    source_tokens_batch = []\n",
    "    target_tokens_batch = []\n",
    "    for i in ixs:\n",
    "        source_tokens = ['[CLS]'] + tokenize(source_sentences[i]) + ['[SEP]']\n",
    "        if len(source_tokens) < sequence_max_length:\n",
    "            # 填充（PAD）并创建注意力掩码\n",
    "            padding_length_source = sequence_max_length - len(source_tokens)\n",
    "            source_tokens += [sequence_empty_pad] * padding_length_source\n",
    "        source_tokens_batch.append(source_tokens)\n",
    "\n",
    "        target_tokens = ['[CLS]'] + tokenize(target_sentences[i]) + ['[SEP]']\n",
    "        if len(target_tokens) < sequence_max_length:\n",
    "            # 填充（PAD）并创建注意力掩码\n",
    "            padding_length_target = sequence_max_length - len(target_tokens)\n",
    "            target_tokens += [sequence_empty_pad] * padding_length_target\n",
    "        target_tokens_batch.append(target_tokens)\n",
    "\n",
    "    return source_tokens_batch, target_tokens_batch\n",
    "\n",
    "\n",
    "x, y = get_tokens_batch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5546f034-38da-44b8-bec6-683ff3477e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 3, 0])\n"
     ]
    }
   ],
   "source": [
    "x, y = get_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "280fd198-96e6-4a3d-8f0b-c67b56f63fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[CLS]',\n",
       "  '铃',\n",
       "  '儿',\n",
       "  '响',\n",
       "  '叮',\n",
       "  '当',\n",
       "  '[SEP]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]'],\n",
       " ['[CLS]', '在', '东', '南', '亚', '打', '自', '由', '搏', '击', '[SEP]', '[PAD]'],\n",
       " ['[CLS]',\n",
       "  '你',\n",
       "  '真',\n",
       "  '是',\n",
       "  '个',\n",
       "  '傻',\n",
       "  '逼',\n",
       "  '[SEP]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]']]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ba945293-6357-470e-ade1-07d715d44775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[CLS]',\n",
       "  '今',\n",
       "  '天',\n",
       "  '真',\n",
       "  '开',\n",
       "  '心',\n",
       "  '[SEP]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]'],\n",
       " ['[CLS]',\n",
       "  '这',\n",
       "  '货',\n",
       "  '真',\n",
       "  '牛',\n",
       "  '逼',\n",
       "  '[SEP]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]'],\n",
       " ['[CLS]',\n",
       "  '你',\n",
       "  '真',\n",
       "  '是',\n",
       "  '个',\n",
       "  '小',\n",
       "  '可',\n",
       "  '爱',\n",
       "  '[SEP]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "41c26f81-858d-4dd0-a630-6bc533261176",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def embedding(tokens_batch):\n",
    "    return torch.tensor([convert_tokens_to_ids(tokens) for tokens in tokens_batch])\n",
    "\n",
    "\n",
    "x_embedding = embedding(x)\n",
    "y_embedding = embedding(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f923629c-929e-42ee-9ed4-5e9b42e2b005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 12])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "56435038-4562-4777-82b9-6f6c3e960939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 12])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ae77c28e-bde5-4832-8d7a-c3bf6e5d1304",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_trg_self_mask(target_len, device=None):\n",
    "    # Prevent leftward information flow in self-attention.\n",
    "    ones = torch.ones(target_len, target_len, dtype=torch.uint8,\n",
    "                      device=device)\n",
    "    t_self_mask = torch.triu(ones, diagonal=1).unsqueeze(0)\n",
    "\n",
    "    return t_self_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bb599a10-47eb-4448-8669-ea3dfdaf1c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_self = create_trg_self_mask(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "356fd9c2-6d54-4218-8c9b-a2ec566fa664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_self.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ae4364bc-fdf9-4b09-8dc1-a7923878c56c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7f5af1f9-b88b-4e82-bf8a-b5ad9a34f8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_attn_subsequent_mask(seq):\n",
    "  '''\n",
    "  Build attention mask matrix for decoder when it autoregressing.\n",
    "\n",
    "  parameters:\n",
    "  seq: [batch, target_len]\n",
    "\n",
    "  return:\n",
    "  subsequent_mask: [batch, target_len, target_len] \n",
    "  '''\n",
    "  attn_shape = [seq.size(0), seq.size(1), seq.size(1)] # [batch, target_len, target_len]\n",
    "  subsequent_mask = np.triu(np.ones(attn_shape), k=1) # [batch, target_len, target_len] \n",
    "  subsequent_mask = torch.from_numpy(subsequent_mask)\n",
    "\n",
    "  return subsequent_mask # [batch, target_len, target_len] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "46a9d1b5-1c61-40ba-ac8e-2b5a59294c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 1., 1.],\n",
       "        [1., 1., 1.]]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c2809da1-6ebd-4b01-b51e-bab18584e74c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([36])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5d0b4770-ca2d-4872-8df5-5e196c036e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 11])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " y_embedding[:, 1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ec9b65-3d4f-4751-8a3d-4f4fd35461ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a6afc25b-3265-422b-bd03-1bcd12b520fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'['",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mconvert_tokens_to_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_start_pad\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 28\u001b[0m, in \u001b[0;36mconvert_tokens_to_ids\u001b[0;34m(tokens)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_tokens_to_ids\u001b[39m(tokens):\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [char_to_index[token_item] \u001b[38;5;28;01mfor\u001b[39;00m token_item \u001b[38;5;129;01min\u001b[39;00m tokens]\n",
      "Cell \u001b[0;32mIn[20], line 28\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_tokens_to_ids\u001b[39m(tokens):\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mchar_to_index\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken_item\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m token_item \u001b[38;5;129;01min\u001b[39;00m tokens]\n",
      "\u001b[0;31mKeyError\u001b[0m: '['"
     ]
    }
   ],
   "source": [
    "convert_tokens_to_ids(sequence_start_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b072413e-d917-4d6c-8087-313e50d45e5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
