{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "e83a30bc-21f5-45c0-ad48-d7e849b23d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "bert_model_id = \"/Users/zhangnengwei/nnnewworkspace/bert-base-chinese\"\n",
    "# 使用BERT的预训练中文词表\n",
    "tokenizer_cn = BertTokenizer.from_pretrained(bert_model_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "17ae6329-ceb7-4e0f-86e0-2983c0348033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备数据\n",
    "# 先用这个数据作为训练数据，模型训练完成之后，\n",
    "# 向模型输入这个source_sentence,模型能输出target_sentence 那就很满足。\n",
    "source_sentence = \"你真是个傻逼\"\n",
    "target_sentence = \"你真是个小可爱\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "aec51130-702c-415d-9dc9-d51909dd05bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 标记化并添加特殊标记\n",
    "# 为啥要添加特殊标记呢？\n",
    "# 是为了数据可以很规整，模型可以训练起来，并且模型可以识别需要的数据。\n",
    "source_tokens = tokenizer_cn.tokenize(source_sentence)\n",
    "source_tokens = ['[CLS]'] + source_tokens + ['[SEP]']\n",
    "\n",
    "target_tokens = tokenizer_cn.tokenize(target_sentence)\n",
    "target_tokens = ['[CLS]'] + target_tokens + ['[SEP]']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "4a4d5dc6-1b57-4aef-b042-fbcf320d074d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', '你', '真', '是', '个', '傻', '逼', '[SEP]']"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "909f52b8-81a1-48e2-9e9a-27c5850ec2e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', '你', '真', '是', '个', '小', '可', '爱', '[SEP]']"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "38a437da-45b7-4298-a89e-6db223f057c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 转换为ID\n",
    "source_ids = tokenizer_cn.convert_tokens_to_ids(source_tokens)\n",
    "target_ids = tokenizer_cn.convert_tokens_to_ids(target_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "51488826-0cc9-4994-b7c2-a32becd538c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 872, 4696, 3221, 702, 1004, 6873, 102]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "55d17aee-a1f5-46dc-85cd-adaaa2ac40a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 872, 4696, 3221, 702, 2207, 1377, 4263, 102]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "001421cd-a3c4-4bf5-96f3-37ecb81fafc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 确定最大长度（例如设置为12）\n",
    "# 为啥设置这个最大长度？\n",
    "# 为了整齐划一，模型可以训练.\n",
    "max_source_length = 12\n",
    "max_target_length = 12\n",
    "\n",
    "# 填充（PAD）并创建注意力掩码\n",
    "padding_length_source = max_source_length - len(source_ids)\n",
    "source_ids = source_ids + [0] * padding_length_source\n",
    "attention_mask_source = [1] * len(source_tokens) + [0] * padding_length_source\n",
    "\n",
    "padding_length_target = max_target_length - len(target_ids)\n",
    "target_ids = target_ids + [0] * padding_length_target\n",
    "attention_mask_target = [1] * len(target_tokens) + [0] * padding_length_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3124bc0d-9a80-434e-b038-1767a0b362cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 872, 4696, 3221, 702, 1004, 6873, 102, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "5673a5cd-0fc9-4ce3-9778-67711ca7e917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "6af24c45-795a-4829-a197-c710dc5db6d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 872, 4696, 3221, 702, 2207, 1377, 4263, 102, 0, 0, 0]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "79a1779c-2414-4444-8b57-39668d4f11cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "8bde70c2-7959-4015-94b1-5f2795bc1e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 转换为PyTorch张量\n",
    "source_ids_tensor = torch.tensor([source_ids])\n",
    "attention_mask_source_tensor = torch.tensor([attention_mask_source])\n",
    "\n",
    "target_ids_tensor = torch.tensor([target_ids])\n",
    "attention_mask_target_tensor = torch.tensor([attention_mask_target])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c88aa5ee-8c8c-4a88-a76c-1b55c7492ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_ids_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "1745dd83-b569-4062-987a-01d272b40944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask_source_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "ca67fb58-210f-4964-b5ad-726fcee87044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_ids_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "1a379f2e-c513-4225-af11-6e248c2a37be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask_target_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "29cc637d-1375-41a4-a821-cf630a886998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里的[1,12] 相当于 [batch_size,seq_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "1bfaf506-eba2-4da4-8c93-8523d70bede3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 定义模型\n",
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, decoder_vocab_size):\n",
    "        super().__init__()\n",
    "        # 现在只看数据流的情况下，直接使用bert模型的编码器和解码器\n",
    "        encoder = BertModel.from_pretrained(bert_model_id)\n",
    "        decoder = BertModel.from_pretrained(bert_model_id)\n",
    "        d_model = decoder.config.hidden_size  # 隐藏层的维度，也就是模型中间分析的特征数量\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.linear = nn.Linear(d_model, decoder_vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask):\n",
    "        encoder_outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # 编码器的输出\n",
    "        encoder_hidden_states = encoder_outputs.last_hidden_state\n",
    "        decoder_outputs = self.decoder(input_ids=decoder_input_ids, attention_mask=decoder_attention_mask,\n",
    "                                       encoder_hidden_states=encoder_hidden_states)\n",
    "        # 解码器的输出\n",
    "        return self.linear(decoder_outputs.last_hidden_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6cfb7ef8-bf24-41b5-bf12-9b4ff8d38938",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 初始化模型\n",
    "model = Seq2SeqModel(tokenizer_cn.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "2f81baf6-a946-4eb8-9857-0a009c97687e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前向传播\n",
    "logits = model(input_ids=source_ids_tensor,\n",
    "               attention_mask=attention_mask_source_tensor,\n",
    "               # 将最后一位干掉，让模型去训练，生成这一位。\n",
    "               decoder_input_ids=target_ids_tensor[:, :-1],\n",
    "               decoder_attention_mask=attention_mask_target_tensor[:, :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "12900deb-0584-49c3-a031-a98e398c33ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ids_shifted = target_ids_tensor[:, 1:].contiguous().view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "05825802-5678-481a-99b2-c38c5f960190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_ids_shifted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "a569d65b-bd72-427b-bd06-bbde26662e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = logits.contiguous().view(-1, tokenizer_cn.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "efdb99c7-6246-473c-ad83-9c1c3ec58014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 21128])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "83a73af8-e4ed-471c-a68a-648d5d5da1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # 忽略填充值0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "2bc68131-a74f-4978-9dfc-3fb6a98697d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算损失\n",
    "loss = criterion(logits, target_ids_shifted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "ce25db87-b2a5-4ec6-b4ce-e570fc5b856f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.1809, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "4a0e2401-f161-46f2-8e5d-afb406aaa1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 到这里一次就运行结束了。\n",
    "# 下面开始循环训练的过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "16d538a3-d2c2-47b9-a205-5e4c6f79194d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "b978c44a-6683-4f1e-9a16-3f3bedc05f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 10.106040000915527\n",
      "Epoch 2/100, Loss: 8.465002059936523\n",
      "Epoch 3/100, Loss: 7.590599536895752\n",
      "Epoch 4/100, Loss: 6.773575305938721\n",
      "Epoch 5/100, Loss: 5.4766340255737305\n",
      "Epoch 6/100, Loss: 4.685584545135498\n",
      "Epoch 7/100, Loss: 3.8850173950195312\n",
      "Epoch 8/100, Loss: 3.201249837875366\n",
      "Epoch 9/100, Loss: 2.6361939907073975\n",
      "Epoch 10/100, Loss: 2.285773515701294\n",
      "Epoch 11/100, Loss: 1.7995291948318481\n",
      "Epoch 12/100, Loss: 1.500985860824585\n",
      "Epoch 13/100, Loss: 1.1916507482528687\n",
      "Epoch 14/100, Loss: 0.8764806985855103\n",
      "Epoch 15/100, Loss: 0.8087360858917236\n",
      "Epoch 16/100, Loss: 0.6433466672897339\n",
      "Epoch 17/100, Loss: 0.5131864547729492\n",
      "Epoch 18/100, Loss: 0.4078104794025421\n",
      "Epoch 19/100, Loss: 0.33131086826324463\n",
      "Epoch 20/100, Loss: 0.2964836657047272\n",
      "Epoch 21/100, Loss: 0.2502151131629944\n",
      "Epoch 22/100, Loss: 0.22394880652427673\n",
      "Epoch 23/100, Loss: 0.1938513219356537\n",
      "Epoch 24/100, Loss: 0.18232785165309906\n",
      "Epoch 25/100, Loss: 0.1472962647676468\n",
      "Epoch 26/100, Loss: 0.1368027925491333\n",
      "Epoch 27/100, Loss: 0.12214130163192749\n",
      "Epoch 28/100, Loss: 0.11464691907167435\n",
      "Epoch 29/100, Loss: 0.10326451063156128\n",
      "Epoch 30/100, Loss: 0.09661736339330673\n",
      "Epoch 31/100, Loss: 0.09081567078828812\n",
      "Epoch 32/100, Loss: 0.08706384152173996\n",
      "Epoch 33/100, Loss: 0.07336615025997162\n",
      "Epoch 34/100, Loss: 0.07640202343463898\n",
      "Epoch 35/100, Loss: 0.0706038847565651\n",
      "Epoch 36/100, Loss: 0.06689143180847168\n",
      "Epoch 37/100, Loss: 0.0618269219994545\n",
      "Epoch 38/100, Loss: 0.061368536204099655\n",
      "Epoch 39/100, Loss: 0.06121306121349335\n",
      "Epoch 40/100, Loss: 0.05465729534626007\n",
      "Epoch 41/100, Loss: 0.05684495344758034\n",
      "Epoch 42/100, Loss: 0.05183771997690201\n",
      "Epoch 43/100, Loss: 0.05009011551737785\n",
      "Epoch 44/100, Loss: 0.050697069615125656\n",
      "Epoch 45/100, Loss: 0.051763929426670074\n",
      "Epoch 46/100, Loss: 0.045967526733875275\n",
      "Epoch 47/100, Loss: 0.046482548117637634\n",
      "Epoch 48/100, Loss: 0.04266129806637764\n",
      "Epoch 49/100, Loss: 0.04630899429321289\n",
      "Epoch 50/100, Loss: 0.041208136826753616\n",
      "Epoch 51/100, Loss: 0.04189138859510422\n",
      "Epoch 52/100, Loss: 0.03966638073325157\n",
      "Epoch 53/100, Loss: 0.03943910822272301\n",
      "Epoch 54/100, Loss: 0.03633808344602585\n",
      "Epoch 55/100, Loss: 0.03726819530129433\n",
      "Epoch 56/100, Loss: 0.03720968961715698\n",
      "Epoch 57/100, Loss: 0.03461768850684166\n",
      "Epoch 58/100, Loss: 0.03446932137012482\n",
      "Epoch 59/100, Loss: 0.03591098263859749\n",
      "Epoch 60/100, Loss: 0.05353131890296936\n",
      "Epoch 61/100, Loss: 0.033210474997758865\n",
      "Epoch 62/100, Loss: 0.03158014267683029\n",
      "Epoch 63/100, Loss: 0.03346841037273407\n",
      "Epoch 64/100, Loss: 0.03239471837878227\n",
      "Epoch 65/100, Loss: 0.03327134624123573\n",
      "Epoch 66/100, Loss: 0.03137434646487236\n",
      "Epoch 67/100, Loss: 0.031451303511857986\n",
      "Epoch 68/100, Loss: 0.030499761924147606\n",
      "Epoch 69/100, Loss: 0.03076334297657013\n",
      "Epoch 70/100, Loss: 0.027316734194755554\n",
      "Epoch 71/100, Loss: 0.030559757724404335\n",
      "Epoch 72/100, Loss: 0.027552438899874687\n",
      "Epoch 73/100, Loss: 0.028683366253972054\n",
      "Epoch 74/100, Loss: 0.028850305825471878\n",
      "Epoch 75/100, Loss: 0.026877008378505707\n",
      "Epoch 76/100, Loss: 0.02709505334496498\n",
      "Epoch 77/100, Loss: 0.026380524039268494\n",
      "Epoch 78/100, Loss: 0.027723941951990128\n",
      "Epoch 79/100, Loss: 0.026458939537405968\n",
      "Epoch 80/100, Loss: 0.0269724503159523\n",
      "Epoch 81/100, Loss: 0.026798462495207787\n",
      "Epoch 82/100, Loss: 0.026181505993008614\n",
      "Epoch 83/100, Loss: 0.024289218708872795\n",
      "Epoch 84/100, Loss: 0.024546926841139793\n",
      "Epoch 85/100, Loss: 0.024719268083572388\n",
      "Epoch 86/100, Loss: 0.025142313912510872\n",
      "Epoch 87/100, Loss: 0.025482239201664925\n",
      "Epoch 88/100, Loss: 0.023488856852054596\n",
      "Epoch 89/100, Loss: 0.0245790034532547\n",
      "Epoch 90/100, Loss: 0.023240627720952034\n",
      "Epoch 91/100, Loss: 0.023844020441174507\n",
      "Epoch 92/100, Loss: 0.023768991231918335\n",
      "Epoch 93/100, Loss: 0.02298589050769806\n",
      "Epoch 94/100, Loss: 0.022278249263763428\n",
      "Epoch 95/100, Loss: 0.023292088881134987\n",
      "Epoch 96/100, Loss: 0.021633215248584747\n",
      "Epoch 97/100, Loss: 0.023311860859394073\n",
      "Epoch 98/100, Loss: 0.02173500508069992\n",
      "Epoch 99/100, Loss: 0.02123093605041504\n",
      "Epoch 100/100, Loss: 0.021000271663069725\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 循环训练\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 前向传播\n",
    "    logits = model(input_ids=source_ids_tensor,\n",
    "                   attention_mask=attention_mask_source_tensor,\n",
    "                   decoder_input_ids=target_ids_tensor[:, :-1],\n",
    "                   decoder_attention_mask=attention_mask_target_tensor[:, :-1])\n",
    "\n",
    "    # 调整目标张量形状以适应损失函数\n",
    "    # contiguous() 方法在 PyTorch 中的作用是确保张量的内存布局是连续的，\n",
    "    # 从而使得一些需要连续内存布局的操作（如 view）能够顺利进行。\n",
    "    # 了解和正确使用 contiguous() 是处理复杂张量操作时的一项重要技能。\n",
    "    # .view(-1)是一种常用的方法，用于将张量展平（flatten）成一个一维张量。\n",
    "    target_ids_shifted = target_ids_tensor[:, 1:].contiguous().view(-1)\n",
    "    logits = logits.contiguous().view(-1, tokenizer_cn.vocab_size)\n",
    "\n",
    "    # 计算损失\n",
    "    loss = criterion(logits, target_ids_shifted)\n",
    "\n",
    "    # 反向传播和优化\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f20a2b62-78e5-401c-b021-9c2e5fa1ad23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型已保存到 seq2seq_model.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 保存模型和优化器状态字典\n",
    "save_path = \"seq2seq_model.pth\"\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, save_path)\n",
    "\n",
    "print(f\"模型已保存到 {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9e85c922-d099-4954-a618-94cb82eb035d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型和优化器已加载\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 加载模型\n",
    "checkpoint = torch.load(save_path)\n",
    "\n",
    "# 重新初始化模型和优化器\n",
    "loaded_model = Seq2SeqModel(tokenizer_cn.vocab_size)\n",
    "loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "loaded_optimizer = optim.Adam(loaded_model.parameters(), lr=0.0001)\n",
    "loaded_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "print(\"模型和优化器已加载\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "93eb73bb-0f14-4c59-aa25-6f5f6401460c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 使用模型推理\n",
    "\n",
    "# 设置模型为评估模式\n",
    "loaded_model.eval()\n",
    "\n",
    "def translate(model, source_sentence, max_target_length=12):\n",
    "    # 将源句子标记化并转换为ID\n",
    "    source_tokens = tokenizer_cn.tokenize(source_sentence)\n",
    "    source_tokens = ['[CLS]'] + source_tokens + ['[SEP]']\n",
    "    source_ids = tokenizer_cn.convert_tokens_to_ids(source_tokens)\n",
    "\n",
    "    # 填充源输入\n",
    "    padding_length_source = max_source_length - len(source_ids)\n",
    "    source_ids = source_ids + [0] * padding_length_source\n",
    "    attention_mask_source = [1] * len(source_tokens) + [0] * padding_length_source\n",
    "\n",
    "    # 转换为PyTorch张量\n",
    "    source_ids_tensor = torch.tensor([source_ids])\n",
    "    attention_mask_source_tensor = torch.tensor([attention_mask_source])\n",
    "\n",
    "    # 初始化解码器输入\n",
    "    decoder_input_ids = torch.tensor([[tokenizer_cn.cls_token_id]])\n",
    "\n",
    "    # 用于存储生成的目标序列\n",
    "    generated_ids = []\n",
    "\n",
    "    for _ in range(max_target_length):\n",
    "        # 前向传播\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids=source_ids_tensor,\n",
    "                           attention_mask=attention_mask_source_tensor,\n",
    "                           decoder_input_ids=decoder_input_ids,\n",
    "                           decoder_attention_mask=torch.ones_like(decoder_input_ids))\n",
    "\n",
    "        # 获取当前时间步的预测结果\n",
    "        # [1, 21128]\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        # 找出概率最大的哪一个\n",
    "        next_token_id = next_token_logits.argmax(dim=-1).item()\n",
    "        print(f\"next_token_id : {next_token_id}\")\n",
    "\n",
    "        # 添加到生成的序列中\n",
    "        generated_ids.append(next_token_id)\n",
    "\n",
    "        # 更新解码器输入\n",
    "        decoder_input_ids = torch.cat([decoder_input_ids, torch.tensor([[next_token_id]])], dim=-1)\n",
    "\n",
    "        # 如果预测到了[SEP]标记，则停止生成\n",
    "        if next_token_id == tokenizer_cn.sep_token_id:\n",
    "            break\n",
    "\n",
    "    # 转换生成的ID为标记\n",
    "    generated_tokens = tokenizer_cn.convert_ids_to_tokens(generated_ids)\n",
    "    return tokenizer_cn.convert_tokens_to_string(generated_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "7219a153-b1db-4b5c-84bc-8ae76599af96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_token_id : 4106\n",
      "next_token_id : 3221\n",
      "next_token_id : 702\n",
      "next_token_id : 2207\n",
      "next_token_id : 1377\n",
      "next_token_id : 4263\n",
      "next_token_id : 102\n",
      "翻译: 瀕 是 个 小 可 爱 [SEP]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 测试\n",
    "source_sentence = \"她是个傻逼\"\n",
    "translation = translate(loaded_model, source_sentence)\n",
    "print(f\"翻译: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45f8d41-be43-450f-bc69-3c6cb593bea4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
