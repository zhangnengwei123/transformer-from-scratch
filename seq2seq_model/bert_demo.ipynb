{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e83a30bc-21f5-45c0-ad48-d7e849b23d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "bert_model_id = \"/Users/zhangnengwei/nnnewworkspace/bert-base-chinese\"\n",
    "# 使用BERT的预训练中文词表\n",
    "tokenizer_cn = BertTokenizer.from_pretrained(bert_model_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17ae6329-ceb7-4e0f-86e0-2983c0348033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备数据\n",
    "# 先用这个数据作为训练数据，模型训练完成之后，\n",
    "# 向模型输入这个source_sentence,模型能输出target_sentence 那就很满足。\n",
    "source_sentence = \"你真是个傻逼\"\n",
    "target_sentence = \"你真是个小可爱\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aec51130-702c-415d-9dc9-d51909dd05bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 标记化并添加特殊标记\n",
    "# 为啥要添加特殊标记呢？\n",
    "# 是为了数据可以很规整，模型可以训练起来，并且模型可以识别需要的数据。\n",
    "source_tokens = tokenizer_cn.tokenize(source_sentence)\n",
    "source_tokens = ['[CLS]'] + source_tokens + ['[SEP]']\n",
    "\n",
    "target_tokens = tokenizer_cn.tokenize(target_sentence)\n",
    "target_tokens = ['[CLS]'] + target_tokens + ['[SEP]']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "4a4d5dc6-1b57-4aef-b042-fbcf320d074d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', '你', '真', '是', '个', '傻', '逼', '[SEP]']"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6616dce8-592b-4d6e-94c1-8cc26a6a2fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_cn.cls_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "909f52b8-81a1-48e2-9e9a-27c5850ec2e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', '你', '真', '是', '个', '小', '可', '爱', '[SEP]']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38a437da-45b7-4298-a89e-6db223f057c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 转换为ID\n",
    "source_ids = tokenizer_cn.convert_tokens_to_ids(source_tokens)\n",
    "target_ids = tokenizer_cn.convert_tokens_to_ids(target_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51488826-0cc9-4994-b7c2-a32becd538c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 872, 4696, 3221, 702, 1004, 6873, 102]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55d17aee-a1f5-46dc-85cd-adaaa2ac40a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 872, 4696, 3221, 702, 2207, 1377, 4263, 102]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "001421cd-a3c4-4bf5-96f3-37ecb81fafc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 确定最大长度（例如设置为12）\n",
    "# 为啥设置这个最大长度？\n",
    "# 为了整齐划一，模型可以训练.\n",
    "max_source_length = 12\n",
    "max_target_length = 12\n",
    "\n",
    "# 填充（PAD）并创建注意力掩码\n",
    "padding_length_source = max_source_length - len(source_ids)\n",
    "source_ids = source_ids + [0] * padding_length_source\n",
    "attention_mask_source = [1] * len(source_tokens) + [0] * padding_length_source\n",
    "\n",
    "padding_length_target = max_target_length - len(target_ids)\n",
    "target_ids = target_ids + [0] * padding_length_target\n",
    "attention_mask_target = [1] * len(target_tokens) + [0] * padding_length_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3124bc0d-9a80-434e-b038-1767a0b362cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 872, 4696, 3221, 702, 1004, 6873, 102, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5673a5cd-0fc9-4ce3-9778-67711ca7e917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6af24c45-795a-4829-a197-c710dc5db6d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 872, 4696, 3221, 702, 2207, 1377, 4263, 102, 0, 0, 0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79a1779c-2414-4444-8b57-39668d4f11cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bde70c2-7959-4015-94b1-5f2795bc1e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 转换为PyTorch张量\n",
    "source_ids_tensor = torch.tensor([source_ids])\n",
    "attention_mask_source_tensor = torch.tensor([attention_mask_source])\n",
    "\n",
    "target_ids_tensor = torch.tensor([target_ids])\n",
    "attention_mask_target_tensor = torch.tensor([attention_mask_target])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c88aa5ee-8c8c-4a88-a76c-1b55c7492ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_ids_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1745dd83-b569-4062-987a-01d272b40944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask_source_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca67fb58-210f-4964-b5ad-726fcee87044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_ids_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a379f2e-c513-4225-af11-6e248c2a37be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask_target_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29cc637d-1375-41a4-a821-cf630a886998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里的[1,12] 相当于 [batch_size,seq_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1bfaf506-eba2-4da4-8c93-8523d70bede3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 定义模型\n",
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, decoder_vocab_size):\n",
    "        super().__init__()\n",
    "        # 现在只看数据流的情况下，直接使用bert模型的编码器和解码器\n",
    "        encoder = BertModel.from_pretrained(bert_model_id)\n",
    "        decoder = BertModel.from_pretrained(bert_model_id)\n",
    "        d_model = decoder.config.hidden_size  # 隐藏层的维度，也就是模型中间分析的特征数量\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.linear = nn.Linear(d_model, decoder_vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask):\n",
    "        encoder_outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # 编码器的输出\n",
    "        encoder_hidden_states = encoder_outputs.last_hidden_state\n",
    "        decoder_outputs = self.decoder(input_ids=decoder_input_ids, attention_mask=decoder_attention_mask,\n",
    "                                       encoder_hidden_states=encoder_hidden_states)\n",
    "        # 解码器的输出\n",
    "        return self.linear(decoder_outputs.last_hidden_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6cfb7ef8-bf24-41b5-bf12-9b4ff8d38938",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 初始化模型\n",
    "model = Seq2SeqModel(tokenizer_cn.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f81baf6-a946-4eb8-9857-0a009c97687e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 前向传播\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_ids_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m               \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask_source_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# 将最后一位干掉，让模型去训练，生成这一位。\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m               \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_ids_tensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m               \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask_target_tensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[21], line 15\u001b[0m, in \u001b[0;36mSeq2SeqModel.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask):\n\u001b[0;32m---> 15\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# 编码器的输出\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     encoder_hidden_states \u001b[38;5;241m=\u001b[39m encoder_outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1103\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1096\u001b[0m         extended_attention_mask \u001b[38;5;241m=\u001b[39m _prepare_4d_causal_attention_mask_for_sdpa(\n\u001b[1;32m   1097\u001b[0m             attention_mask,\n\u001b[1;32m   1098\u001b[0m             input_shape,\n\u001b[1;32m   1099\u001b[0m             embedding_output,\n\u001b[1;32m   1100\u001b[0m             past_key_values_length,\n\u001b[1;32m   1101\u001b[0m         )\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1103\u001b[0m         extended_attention_mask \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_4d_attention_mask_for_sdpa\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_length\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1107\u001b[0m     \u001b[38;5;66;03m# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m     \u001b[38;5;66;03m# ourselves in which case we just need to make it broadcastable to all heads.\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m     extended_attention_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_extended_attention_mask(attention_mask, input_shape)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:448\u001b[0m, in \u001b[0;36m_prepare_4d_attention_mask_for_sdpa\u001b[0;34m(mask, dtype, tgt_len)\u001b[0m\n\u001b[1;32m    440\u001b[0m tgt_len \u001b[38;5;241m=\u001b[39m tgt_len \u001b[38;5;28;01mif\u001b[39;00m tgt_len \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m key_value_length\n\u001b[1;32m    442\u001b[0m \u001b[38;5;66;03m# torch.jit.trace, symbolic_trace and torchdynamo with fullgraph=True are unable to capture the controlflow `is_causal=attention_mask is None and q_len > 1`\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# used as an SDPA argument. We keep compatibility with these tracing tools by always using SDPA's `attn_mask` argument in case we are tracing.\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;66;03m# TODO: For dynamo, rather use a check on fullgraph=True once this is possible (https://github.com/pytorch/pytorch/pull/120400).\u001b[39;00m\n\u001b[1;32m    445\u001b[0m is_tracing \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    446\u001b[0m     torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing()\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mask, torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mProxy)\n\u001b[0;32m--> 448\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_dynamo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mis_compiling())\n\u001b[1;32m    449\u001b[0m )\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tracing \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mall(mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    452\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tgt_len \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;66;03m# For query_length == 1, causal attention and bi-directional attention are the same.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py310/lib/python3.10/site-packages/torch/__init__.py:2003\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _lazy_modules:\n\u001b[1;32m   2002\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[0;32m-> 2003\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py310/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py310/lib/python3.10/site-packages/torch/_dynamo/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_frame, eval_frame, resume_execution\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m list_backends, lookup_backend, register_backend\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m callback_handler, on_compile_end, on_compile_start\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py310/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:40\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_python_dispatch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _disable_current_modes\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_traceback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m format_traceback_short\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config, exc, trace_rules\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CompilerFn\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbytecode_analysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m remove_dead_code, remove_pointless_jumps\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py310/lib/python3.10/site-packages/torch/_dynamo/config.py:344\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mgetcwd(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_compile_debug\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;66;03m# [@compile_ignored: debug]\u001b[39;00m\n\u001b[0;32m--> 344\u001b[0m debug_dir_root \u001b[38;5;241m=\u001b[39m \u001b[43mdefault_debug_dir_root\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# [@compile_ignored: debug]\u001b[39;00m\n\u001b[1;32m    347\u001b[0m _save_config_ignore \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepro_after\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepro_level\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskipfiles_inline_module_allowlist\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    354\u001b[0m }\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py310/lib/python3.10/site-packages/torch/_dynamo/config.py:340\u001b[0m, in \u001b[0;36mdefault_debug_dir_root\u001b[0;34m()\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m    337\u001b[0m         tempfile\u001b[38;5;241m.\u001b[39mgettempdir(), getpass\u001b[38;5;241m.\u001b[39mgetuser(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_compile_debug\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    338\u001b[0m     )\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetcwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_compile_debug\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory"
     ]
    }
   ],
   "source": [
    "# 前向传播\n",
    "logits = model(input_ids=source_ids_tensor,\n",
    "               attention_mask=attention_mask_source_tensor,\n",
    "               # 将最后一位干掉，让模型去训练，生成这一位。\n",
    "               decoder_input_ids=target_ids_tensor[:, :-1],\n",
    "               decoder_attention_mask=attention_mask_target_tensor[:, :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12900deb-0584-49c3-a031-a98e398c33ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ids_shifted = target_ids_tensor[:, 1:].contiguous().view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "05825802-5678-481a-99b2-c38c5f960190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_ids_shifted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "a569d65b-bd72-427b-bd06-bbde26662e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = logits.contiguous().view(-1, tokenizer_cn.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "efdb99c7-6246-473c-ad83-9c1c3ec58014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 21128])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "83a73af8-e4ed-471c-a68a-648d5d5da1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # 忽略填充值0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "2bc68131-a74f-4978-9dfc-3fb6a98697d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算损失\n",
    "loss = criterion(logits, target_ids_shifted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "ce25db87-b2a5-4ec6-b4ce-e570fc5b856f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.1809, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "4a0e2401-f161-46f2-8e5d-afb406aaa1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 到这里一次就运行结束了。\n",
    "# 下面开始循环训练的过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "16d538a3-d2c2-47b9-a205-5e4c6f79194d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "b978c44a-6683-4f1e-9a16-3f3bedc05f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 10.106040000915527\n",
      "Epoch 2/100, Loss: 8.465002059936523\n",
      "Epoch 3/100, Loss: 7.590599536895752\n",
      "Epoch 4/100, Loss: 6.773575305938721\n",
      "Epoch 5/100, Loss: 5.4766340255737305\n",
      "Epoch 6/100, Loss: 4.685584545135498\n",
      "Epoch 7/100, Loss: 3.8850173950195312\n",
      "Epoch 8/100, Loss: 3.201249837875366\n",
      "Epoch 9/100, Loss: 2.6361939907073975\n",
      "Epoch 10/100, Loss: 2.285773515701294\n",
      "Epoch 11/100, Loss: 1.7995291948318481\n",
      "Epoch 12/100, Loss: 1.500985860824585\n",
      "Epoch 13/100, Loss: 1.1916507482528687\n",
      "Epoch 14/100, Loss: 0.8764806985855103\n",
      "Epoch 15/100, Loss: 0.8087360858917236\n",
      "Epoch 16/100, Loss: 0.6433466672897339\n",
      "Epoch 17/100, Loss: 0.5131864547729492\n",
      "Epoch 18/100, Loss: 0.4078104794025421\n",
      "Epoch 19/100, Loss: 0.33131086826324463\n",
      "Epoch 20/100, Loss: 0.2964836657047272\n",
      "Epoch 21/100, Loss: 0.2502151131629944\n",
      "Epoch 22/100, Loss: 0.22394880652427673\n",
      "Epoch 23/100, Loss: 0.1938513219356537\n",
      "Epoch 24/100, Loss: 0.18232785165309906\n",
      "Epoch 25/100, Loss: 0.1472962647676468\n",
      "Epoch 26/100, Loss: 0.1368027925491333\n",
      "Epoch 27/100, Loss: 0.12214130163192749\n",
      "Epoch 28/100, Loss: 0.11464691907167435\n",
      "Epoch 29/100, Loss: 0.10326451063156128\n",
      "Epoch 30/100, Loss: 0.09661736339330673\n",
      "Epoch 31/100, Loss: 0.09081567078828812\n",
      "Epoch 32/100, Loss: 0.08706384152173996\n",
      "Epoch 33/100, Loss: 0.07336615025997162\n",
      "Epoch 34/100, Loss: 0.07640202343463898\n",
      "Epoch 35/100, Loss: 0.0706038847565651\n",
      "Epoch 36/100, Loss: 0.06689143180847168\n",
      "Epoch 37/100, Loss: 0.0618269219994545\n",
      "Epoch 38/100, Loss: 0.061368536204099655\n",
      "Epoch 39/100, Loss: 0.06121306121349335\n",
      "Epoch 40/100, Loss: 0.05465729534626007\n",
      "Epoch 41/100, Loss: 0.05684495344758034\n",
      "Epoch 42/100, Loss: 0.05183771997690201\n",
      "Epoch 43/100, Loss: 0.05009011551737785\n",
      "Epoch 44/100, Loss: 0.050697069615125656\n",
      "Epoch 45/100, Loss: 0.051763929426670074\n",
      "Epoch 46/100, Loss: 0.045967526733875275\n",
      "Epoch 47/100, Loss: 0.046482548117637634\n",
      "Epoch 48/100, Loss: 0.04266129806637764\n",
      "Epoch 49/100, Loss: 0.04630899429321289\n",
      "Epoch 50/100, Loss: 0.041208136826753616\n",
      "Epoch 51/100, Loss: 0.04189138859510422\n",
      "Epoch 52/100, Loss: 0.03966638073325157\n",
      "Epoch 53/100, Loss: 0.03943910822272301\n",
      "Epoch 54/100, Loss: 0.03633808344602585\n",
      "Epoch 55/100, Loss: 0.03726819530129433\n",
      "Epoch 56/100, Loss: 0.03720968961715698\n",
      "Epoch 57/100, Loss: 0.03461768850684166\n",
      "Epoch 58/100, Loss: 0.03446932137012482\n",
      "Epoch 59/100, Loss: 0.03591098263859749\n",
      "Epoch 60/100, Loss: 0.05353131890296936\n",
      "Epoch 61/100, Loss: 0.033210474997758865\n",
      "Epoch 62/100, Loss: 0.03158014267683029\n",
      "Epoch 63/100, Loss: 0.03346841037273407\n",
      "Epoch 64/100, Loss: 0.03239471837878227\n",
      "Epoch 65/100, Loss: 0.03327134624123573\n",
      "Epoch 66/100, Loss: 0.03137434646487236\n",
      "Epoch 67/100, Loss: 0.031451303511857986\n",
      "Epoch 68/100, Loss: 0.030499761924147606\n",
      "Epoch 69/100, Loss: 0.03076334297657013\n",
      "Epoch 70/100, Loss: 0.027316734194755554\n",
      "Epoch 71/100, Loss: 0.030559757724404335\n",
      "Epoch 72/100, Loss: 0.027552438899874687\n",
      "Epoch 73/100, Loss: 0.028683366253972054\n",
      "Epoch 74/100, Loss: 0.028850305825471878\n",
      "Epoch 75/100, Loss: 0.026877008378505707\n",
      "Epoch 76/100, Loss: 0.02709505334496498\n",
      "Epoch 77/100, Loss: 0.026380524039268494\n",
      "Epoch 78/100, Loss: 0.027723941951990128\n",
      "Epoch 79/100, Loss: 0.026458939537405968\n",
      "Epoch 80/100, Loss: 0.0269724503159523\n",
      "Epoch 81/100, Loss: 0.026798462495207787\n",
      "Epoch 82/100, Loss: 0.026181505993008614\n",
      "Epoch 83/100, Loss: 0.024289218708872795\n",
      "Epoch 84/100, Loss: 0.024546926841139793\n",
      "Epoch 85/100, Loss: 0.024719268083572388\n",
      "Epoch 86/100, Loss: 0.025142313912510872\n",
      "Epoch 87/100, Loss: 0.025482239201664925\n",
      "Epoch 88/100, Loss: 0.023488856852054596\n",
      "Epoch 89/100, Loss: 0.0245790034532547\n",
      "Epoch 90/100, Loss: 0.023240627720952034\n",
      "Epoch 91/100, Loss: 0.023844020441174507\n",
      "Epoch 92/100, Loss: 0.023768991231918335\n",
      "Epoch 93/100, Loss: 0.02298589050769806\n",
      "Epoch 94/100, Loss: 0.022278249263763428\n",
      "Epoch 95/100, Loss: 0.023292088881134987\n",
      "Epoch 96/100, Loss: 0.021633215248584747\n",
      "Epoch 97/100, Loss: 0.023311860859394073\n",
      "Epoch 98/100, Loss: 0.02173500508069992\n",
      "Epoch 99/100, Loss: 0.02123093605041504\n",
      "Epoch 100/100, Loss: 0.021000271663069725\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 循环训练\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 前向传播\n",
    "    logits = model(input_ids=source_ids_tensor,\n",
    "                   attention_mask=attention_mask_source_tensor,\n",
    "                   decoder_input_ids=target_ids_tensor[:, :-1],\n",
    "                   decoder_attention_mask=attention_mask_target_tensor[:, :-1])\n",
    "\n",
    "    # 调整目标张量形状以适应损失函数\n",
    "    # contiguous() 方法在 PyTorch 中的作用是确保张量的内存布局是连续的，\n",
    "    # 从而使得一些需要连续内存布局的操作（如 view）能够顺利进行。\n",
    "    # 了解和正确使用 contiguous() 是处理复杂张量操作时的一项重要技能。\n",
    "    # .view(-1)是一种常用的方法，用于将张量展平（flatten）成一个一维张量。\n",
    "    target_ids_shifted = target_ids_tensor[:, 1:].contiguous().view(-1)\n",
    "    logits = logits.contiguous().view(-1, tokenizer_cn.vocab_size)\n",
    "\n",
    "    # 计算损失\n",
    "    loss = criterion(logits, target_ids_shifted)\n",
    "\n",
    "    # 反向传播和优化\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f20a2b62-78e5-401c-b021-9c2e5fa1ad23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型已保存到 seq2seq_model.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 保存模型和优化器状态字典\n",
    "save_path = \"seq2seq_model.pth\"\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, save_path)\n",
    "\n",
    "print(f\"模型已保存到 {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9e85c922-d099-4954-a618-94cb82eb035d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型和优化器已加载\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 加载模型\n",
    "checkpoint = torch.load(save_path)\n",
    "\n",
    "# 重新初始化模型和优化器\n",
    "loaded_model = Seq2SeqModel(tokenizer_cn.vocab_size)\n",
    "loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "loaded_optimizer = optim.Adam(loaded_model.parameters(), lr=0.0001)\n",
    "loaded_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "print(\"模型和优化器已加载\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "93eb73bb-0f14-4c59-aa25-6f5f6401460c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 使用模型推理\n",
    "\n",
    "# 设置模型为评估模式\n",
    "loaded_model.eval()\n",
    "\n",
    "def translate(model, source_sentence, max_target_length=12):\n",
    "    # 将源句子标记化并转换为ID\n",
    "    source_tokens = tokenizer_cn.tokenize(source_sentence)\n",
    "    source_tokens = ['[CLS]'] + source_tokens + ['[SEP]']\n",
    "    source_ids = tokenizer_cn.convert_tokens_to_ids(source_tokens)\n",
    "\n",
    "    # 填充源输入\n",
    "    padding_length_source = max_source_length - len(source_ids)\n",
    "    source_ids = source_ids + [0] * padding_length_source\n",
    "    attention_mask_source = [1] * len(source_tokens) + [0] * padding_length_source\n",
    "\n",
    "    # 转换为PyTorch张量\n",
    "    source_ids_tensor = torch.tensor([source_ids])\n",
    "    attention_mask_source_tensor = torch.tensor([attention_mask_source])\n",
    "\n",
    "    # 初始化解码器输入\n",
    "    decoder_input_ids = torch.tensor([[tokenizer_cn.cls_token_id]])\n",
    "\n",
    "    # 用于存储生成的目标序列\n",
    "    generated_ids = []\n",
    "\n",
    "    for _ in range(max_target_length):\n",
    "        # 前向传播\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids=source_ids_tensor,\n",
    "                           attention_mask=attention_mask_source_tensor,\n",
    "                           decoder_input_ids=decoder_input_ids,\n",
    "                           decoder_attention_mask=torch.ones_like(decoder_input_ids))\n",
    "\n",
    "        # 获取当前时间步的预测结果\n",
    "        # [1, 21128]\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        # 找出概率最大的哪一个\n",
    "        next_token_id = next_token_logits.argmax(dim=-1).item()\n",
    "        print(f\"next_token_id : {next_token_id}\")\n",
    "\n",
    "        # 添加到生成的序列中\n",
    "        generated_ids.append(next_token_id)\n",
    "\n",
    "        # 更新解码器输入\n",
    "        decoder_input_ids = torch.cat([decoder_input_ids, torch.tensor([[next_token_id]])], dim=-1)\n",
    "\n",
    "        # 如果预测到了[SEP]标记，则停止生成\n",
    "        if next_token_id == tokenizer_cn.sep_token_id:\n",
    "            break\n",
    "\n",
    "    # 转换生成的ID为标记\n",
    "    generated_tokens = tokenizer_cn.convert_ids_to_tokens(generated_ids)\n",
    "    return tokenizer_cn.convert_tokens_to_string(generated_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "7219a153-b1db-4b5c-84bc-8ae76599af96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_token_id : 4106\n",
      "next_token_id : 3221\n",
      "next_token_id : 702\n",
      "next_token_id : 2207\n",
      "next_token_id : 1377\n",
      "next_token_id : 4263\n",
      "next_token_id : 102\n",
      "翻译: 瀕 是 个 小 可 爱 [SEP]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 测试\n",
    "source_sentence = \"她是个傻逼\"\n",
    "translation = translate(loaded_model, source_sentence)\n",
    "print(f\"翻译: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45f8d41-be43-450f-bc69-3c6cb593bea4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
